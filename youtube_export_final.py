"""
YouTube Studio æ‰¹é‡å¯¼å‡ºå·¥å…· - æœ€ç»ˆç‰ˆ
è§£å†³ YouTube æ¯æ¬¡æœ€å¤šå¯¼å‡º12æ¡çš„é™åˆ¶
é€šè¿‡å¤šæ¬¡å¯¼å‡º + æ»šåŠ¨/ç¿»é¡µ + åˆå¹¶å»é‡
"""

import asyncio
import csv
import os
import re
import sys
import zipfile
from datetime import datetime
from pathlib import Path

try:
    from playwright.async_api import async_playwright, Page
except ImportError:
    print("æ­£åœ¨å®‰è£… playwright...")
    os.system(f"{sys.executable} -m pip install playwright")
    os.system(f"{sys.executable} -m playwright install chromium")
    from playwright.async_api import async_playwright, Page


# ==================== é…ç½® ====================
CHROME_DEBUG_PORT = 9222
OUTPUT_DIR = "youtube_exports"
DOWNLOADS_DIR = os.path.join(OUTPUT_DIR, "downloads")
MAX_EXPORT_ROUNDS = 50  # æœ€å¤šå¯¼å‡ºè½®æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
# ==============================================


class YouTubeExporter:
    def __init__(self):
        self.page: Page = None
        self.playwright = None
        self.browser = None
        self.exported_count = 0
        
    async def connect(self) -> bool:
        """è¿æ¥åˆ°å·²æ‰“å¼€çš„ Chrome"""
        print("\nğŸ“Œ è¿æ¥ Chrome...")
        
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.connect_over_cdp(
                f"http://localhost:{CHROME_DEBUG_PORT}"
            )
            
            contexts = self.browser.contexts
            if not contexts:
                return False
            
            for page in contexts[0].pages:
                if "studio.youtube.com" in page.url:
                    self.page = page
                    print(f"   âœ… å·²è¿æ¥: {page.url[:60]}...")
                    return True
            
            if contexts[0].pages:
                self.page = contexts[0].pages[0]
                return True
                
            return False
            
        except Exception as e:
            print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
            return False
    
    async def goto_content_analytics(self):
        """å¯¼èˆªåˆ°å†…å®¹åˆ†æé¡µé¢"""
        print("\nğŸ“Œ å¯¼èˆªåˆ°å†…å®¹åˆ†æé¡µé¢...")
        
        match = re.search(r'/channel/(UC[a-zA-Z0-9_-]+)', self.page.url)
        if match:
            channel_id = match.group(1)
            url = f"https://studio.youtube.com/channel/{channel_id}/analytics/tab-content/period-default"
            await self.page.goto(url, wait_until="networkidle", timeout=30000)
            await asyncio.sleep(2)
            print("   âœ… å·²åˆ°è¾¾å†…å®¹åˆ†æé¡µé¢")
    
    async def get_video_count(self) -> int:
        """è·å–é¡µé¢ä¸Šæ˜¾ç¤ºçš„è§†é¢‘æ€»æ•°"""
        # å°è¯•ä»é¡µé¢ä¸Šæ‰¾åˆ°æ€»æ•°æ˜¾ç¤º
        count = await self.page.evaluate("""
            () => {
                // æŸ¥æ‰¾æ˜¾ç¤ºæ€»æ•°çš„å…ƒç´ ï¼Œå¦‚ "1-12 / 56"
                const texts = document.body.innerText;
                const match = texts.match(/\\d+\\s*[-â€“]\\s*\\d+\\s*\\/\\s*(\\d+)/);
                if (match) return parseInt(match[1]);
                
                // å¤‡é€‰ï¼šè®¡ç®—è¡¨æ ¼è¡Œæ•°
                const rows = document.querySelectorAll('ytcp-video-row, [class*="entity-row"]');
                return rows.length;
            }
        """)
        return count or 0
    
    async def scroll_table_down(self):
        """æ»šåŠ¨è¡¨æ ¼åŒºåŸŸï¼ŒåŠ è½½ä¸‹ä¸€æ‰¹æ•°æ®"""
        await self.page.evaluate("""
            () => {
                // æ‰¾åˆ°è¡¨æ ¼å®¹å™¨å¹¶æ»šåŠ¨
                const containers = [
                    document.querySelector('ytcp-entity-page'),
                    document.querySelector('.style-scope.ytcp-analytics-video-table'),
                    document.querySelector('main'),
                    document.documentElement
                ];
                for (const c of containers) {
                    if (c && c.scrollHeight > c.clientHeight) {
                        c.scrollTop = c.scrollHeight;
                    }
                }
                window.scrollTo(0, document.body.scrollHeight);
            }
        """)
        await asyncio.sleep(1)
    
    async def click_next_page(self) -> bool:
        """å°è¯•ç‚¹å‡»ä¸‹ä¸€é¡µæŒ‰é’®"""
        next_btn = await self.page.query_selector(
            '[aria-label*="ä¸‹ä¸€é¡µ"], [aria-label*="Next"], '
            'button:has-text("ä¸‹ä¸€é¡µ"), button:has-text("Next"), '
            '[icon="chevron_right"], .pagination-next'
        )
        
        if next_btn:
            is_disabled = await next_btn.get_attribute("disabled")
            aria_disabled = await next_btn.get_attribute("aria-disabled")
            
            if not is_disabled and aria_disabled != "true":
                await next_btn.click()
                await asyncio.sleep(2)
                return True
        
        return False
    
    async def export_once(self) -> str:
        """æ‰§è¡Œä¸€æ¬¡å¯¼å‡ºï¼Œè¿”å›ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„"""
        os.makedirs(DOWNLOADS_DIR, exist_ok=True)
        
        # æ‰¾å¯¼å‡ºæŒ‰é’®
        export_btn = await self.page.query_selector('[aria-label*="å¯¼å‡º"], [aria-label*="Export"]')
        if not export_btn:
            export_btn = await self.page.evaluate_handle("""
                () => {
                    for (const btn of document.querySelectorAll('button, ytcp-button')) {
                        const text = (btn.textContent + (btn.getAttribute('aria-label') || '')).toLowerCase();
                        if (text.includes('å¯¼å‡º') || text.includes('export')) return btn;
                    }
                    return null;
                }
            """)
        
        if not export_btn:
            return None
        
        # ç‚¹å‡»å¯¼å‡ºæŒ‰é’®
        await export_btn.click()
        await asyncio.sleep(1)
        
        # æ‰¾ CSV é€‰é¡¹
        csv_option = await self.page.query_selector(
            '[role="menuitem"]:has-text("CSV"), '
            '[role="menuitem"]:has-text("å¯¼å‡ºå½“å‰è§†å›¾"), '
            '[role="menuitem"]:has-text("Export current view")'
        )
        
        if not csv_option:
            csv_option = await self.page.evaluate_handle("""
                () => {
                    for (const item of document.querySelectorAll('[role="menuitem"], tp-yt-paper-item')) {
                        const text = item.textContent.toLowerCase();
                        if (text.includes('csv') || text.includes('å¯¼å‡º') || text.includes('export')) {
                            return item;
                        }
                    }
                    return null;
                }
            """)
        
        if not csv_option:
            await self.page.keyboard.press("Escape")
            return None
        
        # ç‚¹å‡»ä¸‹è½½
        try:
            async with self.page.expect_download(timeout=30000) as download_info:
                await csv_option.click()
            
            download = await download_info.value
            filename = download.suggested_filename
            filepath = os.path.join(DOWNLOADS_DIR, f"{datetime.now().strftime('%H%M%S')}_{filename}")
            await download.save_as(filepath)
            return filepath
            
        except Exception as e:
            print(f"   ä¸‹è½½å‡ºé”™: {e}")
            return None
    
    async def export_all(self):
        """å¾ªç¯å¯¼å‡ºæ‰€æœ‰æ•°æ®"""
        print("\nğŸ“Œ å¼€å§‹æ‰¹é‡å¯¼å‡º...")
        
        total_videos = await self.get_video_count()
        print(f"   æ£€æµ‹åˆ°çº¦ {total_videos} ä¸ªè§†é¢‘")
        
        if total_videos > 12:
            print(f"   éœ€è¦å¤šæ¬¡å¯¼å‡ºï¼ˆæ¯æ¬¡æœ€å¤š12æ¡ï¼‰")
            estimated_rounds = (total_videos + 11) // 12
            print(f"   é¢„è®¡éœ€è¦ {estimated_rounds} è½®å¯¼å‡º")
        
        downloaded_files = []
        
        for round_num in range(1, MAX_EXPORT_ROUNDS + 1):
            print(f"\n   ğŸ“¥ ç¬¬ {round_num} è½®å¯¼å‡º...", end=" ")
            
            filepath = await self.export_once()
            
            if filepath:
                downloaded_files.append(filepath)
                self.exported_count += 1
                print(f"âœ… æˆåŠŸ")
            else:
                print(f"âŒ å¤±è´¥")
                break
            
            # å°è¯•ç¿»é¡µæˆ–æ»šåŠ¨åˆ°ä¸‹ä¸€æ‰¹
            has_next = await self.click_next_page()
            
            if not has_next:
                # æ²¡æœ‰ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå°è¯•æ»šåŠ¨
                await self.scroll_table_down()
                await asyncio.sleep(1)
                
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ•°æ®
                new_count = await self.get_video_count()
                if new_count <= total_videos and round_num * 12 >= total_videos:
                    print(f"\n   âœ… å·²å¯¼å‡ºæ‰€æœ‰æ•°æ®")
                    break
            
            await asyncio.sleep(1)
        
        print(f"\n   ğŸ“Š å…±å®Œæˆ {self.exported_count} è½®å¯¼å‡º")
        return downloaded_files
    
    async def close(self):
        if self.playwright:
            await self.playwright.stop()


def extract_and_merge(download_dir: str = DOWNLOADS_DIR) -> str:
    """è§£å‹å¹¶åˆå¹¶æ‰€æœ‰ä¸‹è½½çš„æ–‡ä»¶ï¼Œå»é‡"""
    print("\nğŸ“Œ åˆå¹¶æ‰€æœ‰å¯¼å‡ºæ–‡ä»¶...")
    
    if not os.path.exists(download_dir):
        print("   æ²¡æœ‰ä¸‹è½½æ–‡ä»¶")
        return None
    
    all_rows = []
    fieldnames = None
    file_count = 0
    
    for filename in sorted(os.listdir(download_dir)):
        filepath = os.path.join(download_dir, filename)
        
        if filename.endswith('.zip'):
            file_count += 1
            try:
                with zipfile.ZipFile(filepath, 'r') as zf:
                    for name in zf.namelist():
                        if 'è¡¨æ ¼' in name or 'Table' in name:
                            with zf.open(name) as f:
                                content = f.read().decode('utf-8-sig')
                                lines = content.strip().split('\n')
                                reader = csv.DictReader(lines)
                                
                                if not fieldnames:
                                    fieldnames = reader.fieldnames
                                
                                for row in reader:
                                    first_val = list(row.values())[0] if row else ""
                                    if first_val.lower() not in ['total', 'æ€»è®¡', 'åˆè®¡']:
                                        all_rows.append(dict(row))
            except Exception as e:
                print(f"   âš ï¸ å¤„ç† {filename} å‡ºé”™: {e}")
        
        elif filename.endswith('.csv') and not filename.startswith('youtube_all'):
            file_count += 1
            try:
                with open(filepath, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    if not fieldnames:
                        fieldnames = reader.fieldnames
                    for row in reader:
                        first_val = list(row.values())[0] if row else ""
                        if first_val.lower() not in ['total', 'æ€»è®¡', 'åˆè®¡']:
                            all_rows.append(dict(row))
            except Exception as e:
                print(f"   âš ï¸ å¤„ç† {filename} å‡ºé”™: {e}")
    
    print(f"   å¤„ç†äº† {file_count} ä¸ªæ–‡ä»¶ï¼Œå…± {len(all_rows)} è¡ŒåŸå§‹æ•°æ®")
    
    if not all_rows or not fieldnames:
        print("   æ²¡æœ‰æ•°æ®å¯åˆå¹¶")
        return None
    
    # å»é‡
    seen = set()
    unique_rows = []
    
    # æ‰¾åˆ°ç”¨äºå»é‡çš„åˆ—ï¼ˆè§†é¢‘æ ‡é¢˜æˆ– IDï¼‰
    id_col = None
    for col in ['Content', 'å†…å®¹', 'Video title', 'è§†é¢‘æ ‡é¢˜', 'video_id']:
        if col in fieldnames:
            id_col = col
            break
    
    for row in all_rows:
        if id_col:
            key = row.get(id_col, '')
        else:
            key = str(sorted(row.items()))
        
        if key and key not in seen:
            seen.add(key)
            unique_rows.append(row)
    
    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = os.path.join(OUTPUT_DIR, f"youtube_all_videos_{timestamp}.csv")
    
    with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(unique_rows)
    
    print(f"\n   âœ… åˆå¹¶å®Œæˆ!")
    print(f"   ğŸ“ æ–‡ä»¶: {output_path}")
    print(f"   ğŸ“Š å…± {len(unique_rows)} æ¡è®°å½•ï¼ˆå»é‡åï¼‰")
    
    return output_path


async def main():
    print("\n" + "=" * 60)
    print("   ğŸ“Š YouTube Studio æ‰¹é‡å¯¼å‡ºå·¥å…·")
    print("   è§£å†³æ¯æ¬¡æœ€å¤šå¯¼å‡º12æ¡çš„é™åˆ¶")
    print("=" * 60)
    
    exporter = YouTubeExporter()
    
    try:
        if not await exporter.connect():
            print("\nâŒ æ— æ³•è¿æ¥ Chrome")
            print("   1. è¿è¡Œ start_chrome.bat å¯åŠ¨ Chrome")
            print("   2. ç™»å½• YouTube Studio")
            print("   3. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
            return
        
        await exporter.goto_content_analytics()
        
        print("\n" + "-" * 60)
        print("ğŸ“‹ è¯·åœ¨ Chrome ä¸­:")
        print("   1. ç¡®è®¤å·²åœ¨ 'åˆ†æ > å†…å®¹' é¡µé¢")
        print("   2. è®¾ç½®å¥½æ—¶é—´èŒƒå›´")
        print("   3. è„šæœ¬ä¼šè‡ªåŠ¨å¤šæ¬¡å¯¼å‡ºå¹¶åˆå¹¶")
        print("-" * 60)
        input("\nå‡†å¤‡å¥½åæŒ‰ Enter å¼€å§‹...")
        
        # æ‰¹é‡å¯¼å‡º
        await exporter.export_all()
        
        # åˆå¹¶æ‰€æœ‰æ–‡ä»¶
        extract_and_merge()
    
    finally:
        await exporter.close()
    
    print("\n" + "=" * 60)
    print("   âœ… å®Œæˆ!")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    asyncio.run(main())
